{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyP1Tdt8y9Sg8OJnWs/lie22"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["pip install transformers bitsandbytes accelerate librosa pydub pyannote.audio whisperx jiwer"],"metadata":{"id":"LTSG1dXQD1Dt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!bash"],"metadata":{"id":"i_JPUSnRD8bO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cp /content/drive/MyDrive/whisper/alignment.py /usr/local/lib/python3.10/dist-packages/whisperx/alignment.py"],"metadata":{"id":"r-5hPLDJD66C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install --target='/content/drive/MyDrive/whisper_model/env' transformers whisperx bitsandbytes pyannote.audio jiwer pydub accelerate"],"metadata":{"id":"F9XMnjT93tz7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oynJOrGTsUoP","executionInfo":{"status":"ok","timestamp":1728887384952,"user_tz":-540,"elapsed":24243,"user":{"displayName":"3Team SKN02_Final","userId":"11845071725288629089"}},"outputId":"81d379bc-a5a4-4896-863d-4a6b4b8b30f6"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import sys\n","sys.path.append('/content/drive/MyDrive/whisper_model/env')"],"metadata":{"id":"Rz3LEPG7sTB2","executionInfo":{"status":"ok","timestamp":1728887388528,"user_tz":-540,"elapsed":3,"user":{"displayName":"3Team SKN02_Final","userId":"11845071725288629089"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":8,"metadata":{"id":"ZIlwLdckDqFB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728890574356,"user_tz":-540,"elapsed":102141,"user":{"displayName":"3Team SKN02_Final","userId":"11845071725288629089"}},"outputId":"73c87629-adb3-401c-c6ba-a420431048ce"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading models\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","Some weights of the model checkpoint at kresnik/wav2vec2-large-xlsr-korean were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n","- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at kresnik/wav2vec2-large-xlsr-korean and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/content/drive/MyDrive/whisper_model/env/pytorch_lightning/utilities/migration/migration.py:208: You have multiple `ModelCheckpoint` callback states in this checkpoint, but we found state keys that would end up colliding with each other after an upgrade, which means we can't differentiate which of your checkpoint callbacks needs which states. At least one of your `ModelCheckpoint` callbacks will not be able to reload the state.\n","INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.1.3 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/pyannote/models--pyannote--segmentation/snapshots/059e96f964841d40f1a5e755bb7223f76666bba4/pytorch_model.bin`\n"]},{"output_type":"stream","name":"stdout","text":["Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n","Model was trained with torch 1.7.1, yours is 2.4.1+cu121. Bad things might happen unless you revert torch to 1.x.\n","Performing VAD on full audio\n","Detected 198 speech segments in full audio.\n","Starting speaker diarization\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/MyDrive/whisper_model/env/pyannote/audio/models/blocks/pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1808.)\n","  std = sequences.std(dim=-1, correction=1)\n"]},{"output_type":"stream","name":"stdout","text":["Speaker diarization completed\n","Splitting audio into 10-minute chunks\n","Audio split into 2 chunks\n","Processing chunk 1/2\n","Processing chunk\n","Performing VAD on chunk\n","Detected 13 speech segments in chunk.\n","Starting ASR transcription\n","ASR transcription completed\n","Starting WhisperX post-processing\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Aligned 226 segments.\n","WhisperX post-processing completed\n","Processing chunk 2/2\n","Processing chunk\n","Performing VAD on chunk\n","Detected 8 speech segments in chunk.\n","Starting ASR transcription\n","ASR transcription completed\n","Starting WhisperX post-processing\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Aligned 135 segments.\n","WhisperX post-processing completed\n","Matching speakers to transcription segments\n","Speaker matching completed\n","Saving transcriptions to /content/drive/MyDrive/whisper/20240925-회의록노트1.txt\n","Transcriptions saved to '/content/drive/MyDrive/whisper/20240925-회의록노트1.txt'\n","Processing completed\n","알 수 없음:  근데 아무리 봐도 멘토님이 정해주신 거 그렇게 생각이 안 나\n","참여자_02:  뭐 정해주셨지?\n","참여자_02:  회의록?\n","참여자_03:  회의록 그거 막 그냥 그냥\n","알 수 없음:  아 회의록\n","참여자_02:  쓸만한 게\n","참여자_02:  아 회의록을 정해주셨나 근데?\n","참여자_03:  의견을 내주시긴 하지\n","참여자_01:  1안부터 볼까?\n","참여자_01:  응 처음부터 보자\n","참여자_01:  해계도우미\n","참여자_03:  해계도우미가 부처화를 시킬 수는 있거든\n","참여자_01:  근데 돈에 관련된 거 하지 말자\n","참여자_01:  돈에 관련된 거 어쨌든 사람이 확인을 한 번 해야 되고\n","참여자_03:  이게 뭐 멘토님이 말해주기로는\n","참여자_03:  뭔가 이걸로 해서 뭔가\n","참여자_03:  코스트를 뭔가 획기적으로 줄일 수는 없다\n","참여자_03:  약간 이런 말씀을 해주셨기 때문에\n","참여자_03:  최종후보의 후보지 않나\n","알 수 없음:  넘어가야 될 거 같아\n","참여자_03:  뭐 이거는 일단 핸드폰 거치는 게 좀 힘드니까 아예 제기고\n","참여자_03:  3번은 내 생각에는 그냥\n","참여자_03:  이 기업 내부 자료\n","참여자_03:  그니까 기업의 내부 코드를 가지고 와야 되는 문제가 좀 있어서\n","참여자_03:  좀 조금 그런 데에서 좀 어려움이 있지 않을까?\n","참여자_03:  하는 생각이 들어요\n","참여자_03:  그냥 양을 늘리려고 넣었어\n","알 수 없음:  아 오케이 오케이\n","참여자_03:  그 다음 4번?\n","알 수 없음:  결제가 올라가\n","참여자_01:  근데 이거는 그거라고 그랬지\n","참여자_01:  그니까 작성된 문서가 틀에 맞지 않는 경우도 있지만\n","참여자_01:  그 안에 있는 내용 자체가 방향성이라던가\n","참여자_01:  그런 게 다를 수도 있다\n","참여자_01:  그래서 그걸로 결제를 판단하기가 어렵다\n","참여자_01:  보통은 결제가 안 되는 이유는\n","참여자_03:  뭔가 결제 지정선?\n","참여자_03:  그니까 결제 순서를 잘못 지적을 하거나\n","참여자_03:  아니면 뭔가 내용 내에서 뭐가 부족하다 이런 건데\n","참여자_03:  그러면 결제 내용?\n","참여자_03:  어떤 걸 다\n","알 수 없음:  이렇게 어디서나 또 가져와야 되는 문제가 또 생기고\n","알 수 없음:  네 5번째\n","참여자_03:  고고서나 자료에서 우리가 지정한 키워드에서\n","참여자_01:  해당하는 내용 뽑아내서 한번\n","알 수 없음:  근데 그니까 이 의견을 냈었던 이유가\n","참여자_01:  그 엑셀의 필터 같은 기능처럼\n","참여자_01:  그렇게 하면은 우리가 더\n","참여자_01:  직원들이 어떠한 부분에 대해서 문서를 찾거나 할 때\n","참여자_01:  좀 편하지 않을까?\n","참여자_01:  또 있었고\n","참여자_00:  근데 이렇게 생각을 하다 보니까\n","참여자_01:  이런 부분들이 사실 프롬프트 부분에서 넣어지면은\n","참여자_01:  우리가 만든 SLRM에서\n","참여자_01:  더 빠르게 찾을 수 있겠다라는 생각이 들긴 했어요\n","참여자_01:  그니까 어쨌든 보고서나 자료도 있지만\n","참여자_01:  뭐 다른 의견에서도 회의록도 있었고\n","참여자_01:  만약에 통화를 사실 실시간 안 하고\n","참여자_01:  그냥 통화 기록만 가지고 한다고 쳐도\n","참여자_01:  거기서 사실은 키워드만 잘 뽑아낸다면\n","참여자_01:  정말 활용성이 높을 것 같긴 하거든요\n","참여자_01:  그래서 그냥 그런 부분에서 넣어둔 의견이었어요\n","알 수 없음:  이 의견\n","참여자_01:  근데 이거 그러면 어쨌든 뭔가\n","참여자_03:  이 보고서나 자료라는 것을 회의록으로 대치를 시키면\n","참여자_03:  회의록을 찾아내는 것도\n","참여자_00:  기능상으로는 거의 비슷해 보이기는 해서\n","참여자_03:  사실상 그냥 통합할 수 있지\n","참여자_03:  보고서나 자료나 회의록이 같이\n","참여자_02:  왠지 통합될 수 있는 내용인 것 같아요\n","참여자_03:  메인 회신 추천 기능\n","알 수 없음:  이거는 누나가 설명 좀\n","참여자_01:  내가 이걸 넣긴 했는데\n","참여자_03:  자동으로 회사 내부에 있는 자료들을 기반으로\n","참여자_03:  메일을 회신을 해주는 것 같아요\n","참여자_00:  메일을 만들어주는 거라고 생각을 했는데\n","참여자_03:  근데 또 그러면 어쨌든 내부에 뭔가 자료가 있어야 돼\n","참여자_03:  그러니까 회사의 일정이라던가\n","참여자_03:  뭔가 그런 어쨌든 세부적이고\n","참여자_03:  디테일한 자료들이 있어야 돼서 조금 어려울 것 같아요\n","참여자_03:  판단도 해야지\n","참여자_03:  판단도 해야지\n","알 수 없음:  물론 이제 그런 부분들은 생각을 안 할 수는 없는데\n","참여자_01:  근데 사실 실질적으로 이게 기업에 들어가기 전까지는\n","참여자_01:  그냥 우리가 데이터를 지정을 해서\n","참여자_01:  그게 가능하다는 것만 보여주는 게\n","참여자_01:  우리 역할이 아닐까라는 생각도 사실 했거든요\n","참여자_01:  뭔가 회사에 적용되는 게 보여지면\n","참여자_01:  그 회사도 사실 우리 기능을 가지고\n","참여자_01:  자기 회사에 맞춰서 또 쓸 거잖아\n","참여자_01:  나는 이것도 넣어도 괜찮다고 생각을 하는 게\n","참여자_01:  이렇게 해서 누나가 얘기한 것처럼\n","참여자_01:  사실 우리가 이 날짜에 뭐가 있다고\n","참여자_01:  우리가 만약에 데이터를 넣어놨을 때\n","참여자_01:  근데 전화를 이 날짜에 회의를 잡아야 된다라는 전화를 받았으면\n","참여자_01:  자동으로 이제 그날 뭐가 있어서 안 된다라는 메시지를 보내주는 거\n","참여자_01:  나는 괜찮다고 생각해요\n","참여자_01:  결국 내부 자료도 내가 알아야 된다는 거였어요?\n","참여자_02:  근데 이제 그거를 우리가 만들면 되지 않을까?\n","참여자_03:  메일에 이런 이벤트에서 있었던 예산 자료를 보내주세요\n","참여자_03:  이렇게 하면 표로 이렇게 만들어서\n","참여자_03:  표까지는 아니어도 뭔가\n","참여자_03:  그걸 요약을 해서 보내주거나\n","참여자_03:  그런 걸 생각을 했던 건데\n","참여자_03:  처음에 그러면 이제 자료의 범위도 일단 우리가 정해야 돼\n","참여자_03:  뭔가 어떻게 해야 할지\n","참여자_03:  그리고 이걸 메일을 만약에 기밀인데\n","참여자_03:  뭔가 네가 기밀 상황인데\n","참여자_03:  자동 생성을 했다고 해서 안 보내면 되긴 하지만\n","참여자_03:  그런 것도 일단 판단을 할 수 있어야 되나 싶고\n","알 수 없음:  그런 부분, 그런 기능이 들어간다면\n","참여자_01:  사실 똑같이 권한을 줘서\n","참여자_01:  사실 그 권한을 팀장한테 주면 되지 않을까\n","참여자_01:  그러면 좀 복잡해지려나?\n","참여자_01:  아니면 회신하는 거에서\n","참여자_01:  자동 생성 버튼 이렇게 누르면 생성이 되고\n","참여자_03:  이제 그거를 보낼지 말지는 사용자가 정하는 형식으로 가요\n","알 수 없음:  한 번 더 검토를 한 다음에\n","참여자_03:  그것도 괜찮다\n","참여자_01:  어떻게 보면 그 데이터베이스에 있는 내용을\n","참여자_01:  자동으로 찾아주는 느낌이니까\n","참여자_01:  그 부분도 괜찮고\n","참여자_01:  또 메일 생성할 때의 작성\n","참여자_01:  뭐라 그래야 되지?\n","참여자_01:  우리가 일반 대화 느낌으로 얘기를 하면 안 되잖아\n","참여자_01:  사실 공적인 그런 부분도 잡아주는 부분으로 만들면\n","참여자_01:  좀 더 도움이 되지 않을까 생각이 들긴 해요\n","참여자_01:  나는 메일 대화, 메일 요약하는 건 쓸 수는 않을 것 같은데\n","참여자_02:  결국에 앞에 했던 것들, 통합하는 건 다\n","참여자_02:  뭉뚱그려지긴 하는데\n","참여자_02:  보고서 자료, 메일, 회의료\n","참여자_02:  뭔가 메일이면 메일을 하고\n","참여자_03:  아니면 회의면 회의를 하고\n","참여자_03:  나누어야 될 것 같아\n","참여자_03:  다음주까지 언제까지 할까요?\n","참여자_03:  왜냐하면 화면을 구현해야 하는 문제가 있어서\n","참여자_03: 하면 자동으로 일정이 등록될 수도 있는데\n","참여자_03:  맞지\n","참여자_03:  그것보다는 연일한 일정 등록할까요?\n","참여자_03:  그런 부분도 있고\n","참여자_01:  사실 어느 한계를 정해서\n","참여자_01: 이렇게 해서 등록하기 바람이 많이 불어서\n","참여자_01:  그게 되면 더 하는 건 좋은 건데\n","참여자_01:  그게 낫지 않나.\n","참여자_01:  확인하는 게 좋을 것 같긴 해요.\n","참여자_01:  만약 빨리 된다 하면 더 할 수는 있어\n","참여자_01:  예를 들면 다음주까지 프로젝트 이름 정해오세요 이랬는데 프로젝트 이름 정하고 이런 게 있으면 안 되잖아요.\n","참여자_03:  이렇게 볼 수 있는데\n","참여자_03:  일단 한계라도 제대로 되고\n","참여자_01:  올라가는 모델이 만들어지면\n","참여자_01:  추가적으로 넣어도 될 것 같은\n","참여자_01:  아무래도 오류가 날 수 있으니까 정확하지 않을까.\n","참여자_01:  그래서 약간 그런 느낌이잖아.\n","참여자_01:  그치 아무래도.\n","참여자_01:  만약에 회의록이 있어.\n","참여자_01:  그리고 만약에 일정관리 부분에 대해서 우리가 회의를 했을 때\n","참여자_01:  근데 우리가 회의록으로 만들었는데\n","참여자_01:  됐어.\n","참여자_01:  그럼 회의록 말고 다른 PDF 파일이나 이런 것도 가능하지 않을까 해서\n","참여자_01:  저거 날짜만 봐도 오늘부터 한 10월 25일까지는 학습된 인공지능 모델이라던가\n","참여자_01:  그거를 코드를 몇 줄 추가하고\n","알 수 없음:  모델 추가, 이렇게는 확장할 수 있는데\n","참여자_01:  이러면 자동으로 스타트랑 엔드데이트를 생성할 수 있나?\n","참여자_01:  처음부터 약간\n","참여자_01:  이거 이거 통합하고, 이거 통합하고, 이거 통합하고\n","참여자_01:  해보자 하면\n","참여자_01:  그 기능이 들어가면 왜냐하면 일반적인 스케줄 관리뿐만 아니라\n","알 수 없음:  사실 일단은 그냥 따로따로 보고 할 수 있는데\n","알 수 없음:  일단 메일도 보니까 얘기해볼 만한 거리는 되는 것 같아서\n","참여자_03:  만약에 프로젝트를 할 때 사용하게 된다면 프로젝트 이 기간 동안\n","참여자_03:  후보 정도는 될 것 같은데?\n","참여자_03:  맞지.\n","참여자_03:  메뉴얼을 확실시키면 상담소에다가 이제\n","참여자_01:  되게 편하게 알아서 올라가고 해서 확인할 수 있는 화면까지 만들면\n","참여자_03:  나갔다 온다.\n","참여자_03:  안 나와?\n","참여자_01:  요즘 앱이라 그런가?\n","참여자_01:  나도 이거 지금 온라인으로 확인했어.\n","참여자_01:  아 온라인이라고?\n","참여자_03:  그냥 컨트롤에서 눌렀어.\n","참여자_01:  그것도 되게 도움이 될 것 같아요.\n","참여자_01:  확실하게 되나?\n","참여자_01:  그거 이제 나한테도 보이지.\n","참여자_01:  클린트도 다 날짜만 있는 게 아니라 이렇게 즐거워서 이렇게 있잖아.\n","참여자_01:  무섭는데?\n","참여자_02:  쓸데없는 소리가 자꾸 막 들어가거든.\n","참여자_02:  색깔로 시설 같은 거.\n","참여자_02:  이걸 또 어떻게 근데 메뉴얼을\n","알 수 없음:  그렇게 해서 그게 되면 디데이 며칠도 띄워놔야 되고.\n","알 수 없음:  난 너무 단순한 것 같아.\n","알 수 없음:  근데 이제 만약에 일정관리를 하면 전력 이런 건 어디선가 때워야 될 것 같긴 해.\n","참여자_02:  응.\n","참여자_02:  뭔가\n","참여자_01:  아 이 부분은 뭔가 많이 쓰기보다는\n","참여자_01:  상담이 있는\n","알 수 없음:  잔고에서 전력.\n","참여자_01:  그 부분에서만 사용이 가능한 거 아닌가?\n","참여자_01:  잔고로 다 캘린더 만드는 게 있더라고.\n","참여자_01:  캘린더가 있잖아.\n","참여자_01:  화면에 내보내려고.\n","참여자_01:  상담소?\n","참여자_01:  어.\n","알 수 없음:  뭐 이런 콜센터 뭐 그런 데서만 쓰겠지?\n","참여자_01:  잘 만들어야지.\n","참여자_01:  TK인터 써서 하는 것도 나쁘지 않은데.\n","참여자_01:  그치.\n","알 수 없음:  8번은 뭐\n","알 수 없음:  만드는 거?\n","참여자_03:  TK인터로 하는 게 있던 것 같아.\n","참여자_03:  멘토님의 코멘트\n","참여자_03:  이거지.\n","참여자_01: 이렇게 그냥 만들어주는.\n","참여자_01:  같이 하고\n","참여자_01:  BTOB가 아니라 BTOC여서\n","참여자_01:  맞아.\n","참여자_01: 그거야.\n","참여자_03:  아니면 GUI 프로그래밍으로 프로그램을 만들 수 있을 것 같아.\n","참여자_03:  다 9번\n","참여자_03:  9번은 만약에 진짜 그냥 노동법을 넣어놓고\n","참여자_03:  훨씬 어렵겠지만.\n","참여자_01:  이제 임직원들이 누가 뭐 이런 거 물어보는 채폭지다\n","참여자_01:  제법 고생하겠는데.\n","참여자_03:  뭐 약간 이런 식으로 쓸 거면\n","참여자_03:  이거 쉽지 않은데.\n","참여자_03:  시간 다 생각 안 하고.\n","참여자_03:  너무 7번하고 비슷한 스타일로\n","참여자_03:  어쨌든 지금 아이디어 그냥 이거니까.\n","참여자_03:  맞아.\n","참여자_03:  안 될 것 같고\n","참여자_03:  아니면 간단한 게 마이크로소프트에서 투두 있잖아.\n","참여자_03:  그리고 BTOC한테?\n","참여자_03:  응.\n","참여자_03: 맞아.\n","참여자_03:  진짜 개인적으로 BTOC한테?\n","참여자_03:  그런 느낌이 좀 들고\n","알 수 없음:  만약에 인사팀한테 법규나\n","참여자_03:  원래 프로그래밍 처음 배울 때 하기 좋은 플랫폼이 투두였어.\n","참여자_03:  그런 사내 그거\n","참여자_00:  수정하는 거?\n","참여자_02:  맞아.\n","참여자_02:  뭐 이런 거나\n","참여자_02: 맞아.\n","참여자_03:  이렇게 하고 투두는 팀마다 거기만 하면 좋겠다.\n","참여자_03:  아니면 답변 만들어주는 거\n","참여자_03:  그거는 BTOB이긴 하겠지만\n","참여자_03:  그런데 UI가 예쁘게 나오기가 힘든데\n","참여자_03:  뭔가\n","참여자_03:  그것도 어쨌건 채폭지의 느낌이 더 강하고\n","참여자_03:  투두 안에다가 캘린더 날짜를 씌우는 건 쉽잖아.\n","참여자_03:  뭔가 생산성 있는 뭔가를 할 수가 없는 느낌\n","참여자_03:  차라리 투두로.\n","참여자_03:  뭔가 확장성이 없는 느낌\n","참여자_03:  좋은 것 같아.\n","알 수 없음:  정리도 시작 날짜, 끝 날짜.\n","참여자_03:  회의록이나 이런 주제에서는\n","참여자_03:  괜찮은 것 같아.\n","참여자_03:  괜찮네.\n","참여자_03:  새로 요약된 PDF를 제공을 해준다거나\n","참여자_03:  그럼 투두로 등록을 할지 안 할지.\n","참여자_01:  또 뭐가 있을까?\n","참여자_03:  검색을 할 수 있다던가\n","참여자_03:  이 정도?\n","참여자_03:  11은 추가 캘린더 정도?\n","참여자_03:  뭐 이런 게 있는데\n","참여자_03:  이거는 그냥 단순히 지리온답에서 멈추는 거니까\n","참여자_03:  11은 너무 좋습니다.\n","참여자_03:  우리 프로젝트가 두 달인 걸 감안하면\n","참여자_03:  11은 너무 좋습니다.\n","참여자_03:  회의룡 및 회의 요약 자동성성.\n","참여자_03:  조금 너무 단순한 것 같은 느낌이 좀...\n","참여자_03:  이거를 어떤 방식으로 할 거냐.\n","참여자_03:  음... 오케이.\n","참여자_03: 패스.\n","참여자_03:  일단 앞에 나도 이렇게 생각하는데\n","참여자_03:  그런데 요약이란 이런 거를 우리가 음성을 받아서 텍스트 파일로 바꿔서 하고\n","참여자_03:  일정을 자동으로 등록하기는 하긴 불편한데\n","참여자_00:  일정을 등록할까 하다가\n","알 수 없음:  등록하기, 안 하기, 안 하기\n","참여자_02:  그리고 정리한 사람이나 아이디어를 낸 사람.\n","참여자_02:  그게 있어야 되지 않을까?\n","참여자_02:  그러니까 예를 들면은\n","알 수 없음:  페이록을 이용한 자동이잖아요?\n","참여자_02:  음... 한번 확인할 수 있는 칸을 넣으면 좋다는 거지\n","참여자_02:  거의 안 한다고 그랬지.\n","참여자_03:  그거를 많은 사람들이.\n","참여자_03:  회의에 참여한 대부분의 사람들이 하는 게 아니라\n","알 수 없음:  형 말해\n","참여자_03:  그래서 회의 중에 뭐\n","참여자_01:  다음 주까지 뭐 해야 됩니다?\n","알 수 없음:  거의 대부분 막내가 정리하고 한다고 그랬지.\n","참여자_03: 이러면\n","참여자_01:  그러면 녹음한 거 바꾸는 건 구현 차이인 것 같아.\n","참여자_02:  녹음한 거부터 갈지 안 갈지.\n","참여자_02:  회의룩을 같이 갈지는.\n","참여자_02:  내가 냈던 의견이 있었잖아.\n","참여자_02:  만약에 작성한 회의룩이 있으면 사실은 업로드하는 기차는 보다는\n","참여자_03:  장고에서 바로 처리가 가능하게 할 수 있나.\n","참여자_03:  업로드하는 거랑 장고에서 입력을 하는 거랑 비교했을 때\n","참여자_03:  뭔가 실물을 갔어.\n","참여자_02:  내가 편한 노트를 켜서 그냥 다 입력을 하고\n","참여자_02:  올려.\n","참여자_01:  이거 하고 뭔가 장고 우리가 지정된 어떤 페이지에 들어가서\n","참여자_01:  뭔가 노트를 짜자자자자자 쓰고 저장을 눌러.\n","참여자_01:  이거랑 크게 차이가 없을 것 같긴 해.\n","참여자_01:  만약에 회의룩을 우리가 이렇게 녹음을 해서\n","참여자_01:  음성 분리를 해보려고 했잖아.\n","참여자_01:  근데 그게 잘 안 된다고 하면\n","참여자_01:  회의룩을 장고 페이지에서 입력하는 방식으로 가는 게 맞을 것 같아.\n","참여자_01:  장고에서도 회의룩을 만약에 쓸 때\n","참여자_01:  뭔가 긴 채팅창처럼 그렇게 쓰는 것보다\n","참여자_01:  긴 글 쓰는 블로그처럼 쓸 수 있는 형식으로 가야\n","참여자_02:  좀 편하지 않을까 하는 생각도 들어.\n","참여자_02:  그냥 화면 자체에다가?\n","참여자_01:  중간 스토어에다가?\n","참여자_01:  응.\n","참여자_01:  그거는 누구 사면 되니까.\n","참여자_03:  그런 부분은 저장하기라든가\n","참여자_03:  그런 거 한 개로 데이터베이스로 바로 보내버리면 될 것 같아.\n","참여자_02:  뭔가 회의룩을 쓸 거면 글 단위가 나을 것 같아서\n","참여자_02:  전으로 가서 수정도 막 했다가\n","참여자_03:  위로 왔다가 이렇게 하는 게 나을 것 같아.\n","참여자_03:  괜찮다.\n","참여자_02:  음성 분리가 됐으면 좋겠는데.\n","참여자_02:  쉽지 않을 것 같아.\n","참여자_02:  그러니까 쉽지 않을 것 같아.\n","참여자_02:  어제 찾아봤는데\n","참여자_02:  남자 여자 공감력이 있단다.\n","참여자_03:  뭔가 있긴 있는데\n","참여자_03:  막 활성화되어있진 않은 것 같아 아직.\n","참여자_02:  내가 보니까 거의 최신 기술일 것 같아.\n","참여자_02:  대낮에 AI에 들어가 있는 그런 기술을\n","참여자_02:  우리가 구현할 수 있을까?\n","참여자_03:  아니면 만약에 기업에서 한다 치면\n","참여자_03:  우리 4명이랑 멘토님까지 5명의 목소리를\n","참여자_01:  훈련을 시켜.\n","참여자_01:  그러면 이제 실제 서비스에서 쓰려면\n","참여자_01:  그러면 사람 하나하나를 다 훈련을 시켜야 되냐?\n","참여자_02:  라는 생각이 있지.\n","참여자_02:  기업에서는 쉽지 않을까?\n","참여자_02:  쉽지 않더라.\n","참여자_02:  그럼 이제 훈련을 시켜.\n","참여자_02:  인사를 하세요.\n","참여자_02:  입력을 녹음해야 돼요.\n","참여자_01:  이런 거 읽게끔.\n","참여자_03:  너무 번거롭지 않아야 돼.\n","참여자_03:  처음엔 번거로울지라도\n","참여자_03:  나중에 위해서면\n","참여자_03:  원래 처음 도입은 사실 다 번거롭잖아.\n","참여자_02:  차라리 그냥 목소리 자체만\n","참여자_02:  그냥 객체 자체만 분리가 된다면\n","참여자_02:  1, 2, 3 이렇게\n","참여자_03:  그렇게만 되면\n","참여자_03:  이거 기록하는 거 담당하는 사람이\n","알 수 없음:  어쨌건 있을 거 아니야.\n","참여자_01:  사용자 1, 2, 3, 4라고 기록된 거를\n","참여자_01:  누구 대리님, 누구 사원님, 누구 팀장님\n","참여자_01:  이렇게 갈아치우기를 하면 되잖아.\n","참여자_01:  누군가가.\n","참여자_01:  스토리는 클러스토리는 될 것 같아.\n","참여자_01:  여기까지.\n","참여자_01:  여기까지.\n","\n"]}],"source":["import os\n","from pydub import AudioSegment\n","from faster_whisper import WhisperModel\n","import whisperx\n","from pyannote.audio import Pipeline\n","import torch\n","import numpy as np\n","\n","def perform_vad_on_full_audio(input_audio, vad_pipeline, target_sampling_rate=16000):\n","    print(\"Performing VAD on full audio\")\n","    audio = AudioSegment.from_wav(input_audio)\n","    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","    vad_pipeline = vad_pipeline.to(device)\n","\n","    # 오디오 세그맨트를 넘파이로 바꿔준다.\n","    samples = np.array(audio.get_array_of_samples()).astype(np.float32)\n","\n","    # 형태를 (채널, 시간) 형식으로 만든다.\n","    if audio.channels == 2:\n","        samples = samples.reshape(-1, 2).T\n","    else:\n","        samples = samples.reshape(1, -1)\n","\n","    # 들어가는 datatype이 tensor가 되어야 되기 때문에 바꿔준다.\n","    waveform = torch.from_numpy(samples)\n","\n","    try:\n","        vad = vad_pipeline({\"waveform\": waveform, \"sample_rate\": audio.frame_rate})\n","    except Exception as e:\n","        print(f\"Error performing VAD: {e}\")\n","        return None\n","\n","    speech_segments = vad.get_timeline().support()\n","    print(f\"Detected {len(speech_segments)} speech segments in full audio.\")\n","\n","    speech_audio = AudioSegment.empty()\n","    for segment in speech_segments:\n","        start_ms = int(segment.start * 1000)\n","        end_ms = int(segment.end * 1000)\n","        speech_audio += audio[start_ms:end_ms]\n","\n","    speech_audio = speech_audio.set_frame_rate(target_sampling_rate).set_channels(1).set_sample_width(2)\n","\n","    return speech_audio\n","\n","def split_audio(audio, chunk_duration=600000):  # 600000 ms = 10 minutes\n","    print(\"Splitting audio into 10-minute chunks\")\n","    duration = len(audio)\n","    chunks = [audio[i:i+chunk_duration] for i in range(0, duration, chunk_duration)]\n","    print(f\"Audio split into {len(chunks)} chunks\")\n","    return chunks\n","\n","def perform_vad_on_chunk(audio_chunk, vad_pipeline):\n","    print(\"Performing VAD on chunk\")\n","    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","    vad_pipeline = vad_pipeline.to(device)\n","\n","    # 오디오 세그맨트를 넘파이로 바꿔준다\n","    samples = np.array(audio_chunk.get_array_of_samples()).astype(np.float32)\n","\n","   # 형태를 (채널, 시간) 형식으로 만든다.\n","    if audio_chunk.channels == 2:\n","        samples = samples.reshape(-1, 2).T\n","    else:\n","        samples = samples.reshape(1, -1)\n","\n","   # 들어가는 datatype이 tensor가 되어야 되기 때문에 바꿔준다.\n","    waveform = torch.from_numpy(samples)\n","\n","    try:\n","        vad = vad_pipeline({\"waveform\": waveform, \"sample_rate\": audio_chunk.frame_rate})\n","    except Exception as e:\n","        print(f\"Error performing VAD on chunk: {e}\")\n","        return None\n","\n","    speech_segments = vad.get_timeline().support()\n","    print(f\"Detected {len(speech_segments)} speech segments in chunk.\")\n","\n","    speech_audio = AudioSegment.empty()\n","    for segment in speech_segments:\n","        start_ms = int(segment.start * 1000)\n","        end_ms = int(segment.end * 1000)\n","        speech_audio += audio_chunk[start_ms:end_ms]\n","\n","    return speech_audio\n","\n","def transcribe_audio(model, audio_segment):\n","    print(\"Starting ASR transcription\")\n","    try:\n","        # 오디오 세그맨트를 넘파이로 바꿔준다\n","        audio_array = np.array(audio_segment.get_array_of_samples()).astype(np.float32) / 32768.0\n","        segments, _ = model.transcribe(audio_array, beam_size=5)\n","        segments = list(segments)\n","        print(\"ASR transcription completed\")\n","        transcription_segments = [{\"start\": seg.start, \"end\": seg.end, \"text\": seg.text} for seg in segments]\n","        return transcription_segments\n","    except Exception as e:\n","        print(f\"Error during ASR transcription: {e}\")\n","        return None\n","\n","def post_process_whisperx(transcription_segments, audio_segment, align_model, metadata, device='cuda'):\n","    print(\"Starting WhisperX post-processing\")\n","    try:\n","        # 오디오 세그맨트를 넘파이로 바꿔준다\n","        audio_array = np.array(audio_segment.get_array_of_samples()).astype(np.float32) / 32768.0\n","        result_aligned = whisperx.align(transcription_segments, align_model, metadata, audio_array, device=device)\n","        if isinstance(result_aligned, dict):\n","            aligned_segments = result_aligned.get(\"segments\", [])\n","        else:\n","            print(\"Unknown return format from WhisperX\")\n","            aligned_segments = []\n","\n","        print(f\"Aligned {len(aligned_segments)} segments.\")\n","        print(\"WhisperX post-processing completed\")\n","        return aligned_segments\n","    except Exception as e:\n","        print(f\"Error during WhisperX post-processing: {e}\")\n","        return None\n","\n","def perform_diarization(vad_audio, pipeline, num_speakers):\n","    print(\"Starting speaker diarization\")\n","    try:\n","        # 오디오 세그맨트를 넘파이로 바꿔준다\n","        audio_array = np.array(vad_audio.get_array_of_samples()).astype(np.float32) / 32768.0\n","\n","        # 형태를 (채널, 시간) 형식으로 만든다.\n","        if vad_audio.channels == 2:\n","            audio_array = audio_array.reshape(-1, 2).T\n","        else:\n","            audio_array = audio_array.reshape(1, -1)\n","\n","        diarization = pipeline({\"waveform\": torch.from_numpy(audio_array), \"sample_rate\": vad_audio.frame_rate}, num_speakers=num_speakers)\n","        print(\"Speaker diarization completed\")\n","        return diarization\n","    except Exception as e:\n","        print(f\"Error during speaker diarization: {e}\")\n","        return None\n","\n","def match_speaker_to_segments(diarization, transcription_segments):\n","    print(\"Matching speakers to transcription segments\")\n","    matched_segments = []\n","\n","    for segment in transcription_segments:\n","        midpoint = (segment['start'] + segment['end']) / 2\n","        speaker_found = False\n","        for turn, _, speaker in diarization.itertracks(yield_label=True):\n","            if turn.start <= midpoint <= turn.end:\n","                matched_segments.append((segment['start'], segment['end'], f\"참여자{speaker[7:]}\", segment['text']))\n","                speaker_found = True\n","                break\n","        if not speaker_found:\n","            matched_segments.append((segment['start'], segment['end'], \"알 수 없음\", segment['text']))\n","\n","    matched_segments.sort(key=lambda x: x[0])\n","    print(\"Speaker matching completed\")\n","    return matched_segments\n","\n","def save_transcriptions(matched_segments, output_file):\n","    print(f\"Saving transcriptions to {output_file}\")\n","    try:\n","        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n","            for segment in matched_segments:\n","                start_time, end_time, speaker, text = segment\n","                f.write(f\"{speaker}: {text}\\n\")\n","        print(f\"Transcriptions saved to '{output_file}'\")\n","    except Exception as e:\n","        print(f\"Error saving transcriptions: {e}\")\n","\n","def process_chunk(chunk, whisper_model, align_model, metadata, vad_pipeline):\n","    print(f\"Processing chunk\")\n","\n","    # VAD로 오디오 파일 전처리를 한다.( 빈 음성 부분을 제거한다. )\n","    vad_audio = perform_vad_on_chunk(chunk, vad_pipeline)\n","    if vad_audio is None:\n","        return None\n","\n","    # ASR 전사처리를 한다.\n","    transcription_segments = transcribe_audio(whisper_model, vad_audio)\n","    if transcription_segments is None:\n","        return None\n","\n","    # WhisperX로 후처리한다.\n","    aligned_segments = post_process_whisperx(transcription_segments, vad_audio, align_model, metadata)\n","    if aligned_segments is None:\n","        return None\n","\n","    return aligned_segments\n","\n","def main(input_audio, num_speakers, output_file, device='cuda'):\n","    print(\"Loading models\")\n","    compute_type = \"float16\" if device == 'cuda' else \"float32\"\n","    whisper_model = WhisperModel(\"large-v2\", device=device, compute_type=compute_type)\n","\n","    align_model, metadata = whisperx.load_align_model(language_code='ko', device=device)\n","\n","    vad_pipeline = Pipeline.from_pretrained(\n","        \"pyannote/voice-activity-detection\",\n","        use_auth_token='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'\n","    )\n","\n","    diarization_pipeline = Pipeline.from_pretrained(\n","        \"pyannote/speaker-diarization-3.1\",\n","        use_auth_token='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'\n","    )\n","    diarization_pipeline.to(torch.device(device))\n","\n","    # 전체 오디오 파일을 VAD한 파일을 이용해서 화자분리를 진행한다.\n","    vad_audio = perform_vad_on_full_audio(input_audio, vad_pipeline)\n","    if vad_audio is None:\n","        print(\"VAD on full audio failed. Exiting.\")\n","        return\n","\n","\n","    diarization = perform_diarization(vad_audio, diarization_pipeline, num_speakers)\n","    if diarization is None:\n","        print(\"Diarization failed. Exiting.\")\n","        return\n","\n","    # 청크로 나눈다.\n","    chunks = split_audio(vad_audio)\n","\n","    all_aligned_segments = []\n","    for i, chunk in enumerate(chunks):\n","        print(f\"Processing chunk {i+1}/{len(chunks)}\")\n","        chunk_segments = process_chunk(chunk, whisper_model, align_model, metadata, vad_pipeline)\n","        if chunk_segments:\n","            chunk_duration = len(chunk) / 1000  # chunk duration in seconds\n","            for seg in chunk_segments:\n","                seg['start'] += i * chunk_duration\n","                seg['end'] += i * chunk_duration\n","            all_aligned_segments.extend(chunk_segments)\n","\n","    matched_segments = match_speaker_to_segments(diarization, all_aligned_segments)\n","\n","    save_transcriptions(matched_segments, output_file)\n","\n","    print(\"Processing completed\")\n","\n","    def Sample(output_text_file):\n","      data = open()\n","\n","if __name__ == \"__main__\":\n","    input_audio_file = \"/content/drive/MyDrive/whisper/20240925-회의록.wav\"\n","    num_speakers = 4\n","    output_text_file = \"/content/drive/MyDrive/whisper/20240925-회의록노트1.txt\"\n","\n","    main(\n","        input_audio=input_audio_file,\n","        num_speakers=num_speakers,\n","        device='cuda' if torch.cuda.is_available() else 'cpu',\n","        output_file=output_text_file\n","    )\n","#저장된 회의록 텍스트 파일을 눈으로 결과물 확인을 위해 읽어온다.\n","#read를 통해서 문자열로 불러온다.\n","data = open('/content/drive/MyDrive/whisper/20240925-회의록노트1.txt', 'r', encoding=\"UTF8\")\n","contents = data.read()\n","print(contents)\n","data.close()"]},{"cell_type":"code","source":[],"metadata":{"id":"0T1ayUGZyAup"},"execution_count":null,"outputs":[]}]}